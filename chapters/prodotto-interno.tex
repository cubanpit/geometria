\chapter{Spazi con prodotto interno} \label{ch:spazi-prodotto-interno}
Negli spazi vettoriali abbiamo trattato finora soltanto le operazioni di somma e di prodotto con uno scalare.
Introduciamo ora un'altra operazione, detta \emph{prodotto interno}, che associa a due elementi dello spazio vettoriale uno scalare del campo.
In questo capitolo indicheremo con $K$ sempre il campo dei numeri reali o dei numeri complessi.
Il più noto esempio di prodotto interno è il prodotto scalare tra due vettori di $\R^n$, che restituisce un numero reale: questo è solo uno dei tanti tipi di prodotto interno che si possono costruire in uno spazio vettoriale.
\section{Forme bilineari} \label{sec:forme-bilineari}
Un prodotto interno è innanzitutto una forma bilineare, ossia un'applicazione lineare in entrambe le variabili.
\begin{definizione} \label{d:forma-bilineare}
	Sia $V$ uno spazio vettoriale sul campo $K$.
	Un'applicazione $g\colon V\times V\to K$ è detta \emph{forma bilineare} su $V$ se rispetta le seguenti proprietà:
	\begin{itemize}
		\item per ogni $  v_1,  v_2,  w_1,  w_2\in V$ si ha $g(  v_1+  v_2,  w_1)=g(  v_1,  w_1)+g(  v_2,  w_1)$ e $g(  v_1,  w_1+  w_2)=g(  v_1,  w_1)+g(  v_1+  w_2)$;
		\item per ogni $  v,  w\in V$ e $\lambda,\mu\in K$ si ha $g(\lambda  v,  w)=\lambda g(  v,  w)$ e $g(  v,\mu  w)=\mu g(  v,  w)$
	\end{itemize}
	ossia è lineare nella prima e nella seconda variabile.
\end{definizione}
Si può osservare che, fissata una delle due variabili, le applicazioni $g(  x,\cdot)$ e $g(\cdot,  x)$ sono dei funzionali lineari, cioè appartengono a $V^*$.
La forma bilineare si dice inoltre \emph{simmetrica} se $g(  x,  y)=g(  y,  x)$ qualunque siano $  x,  y\in V$.
A partire da questo diamo ora degli assiomi di definizione dei prodotti interni.

\section{Prodotto interno}	\label{ref:prodotto-interno}
\begin{definizione} \label{d:prodotto-interno}
	Sia $V$ uno spazio vettoriale su $K$.
	Si definisce \emph{prodotto interno} su $V$ l'applicazione $\intern{}{}\colon V\to K$ che gode delle seguente proprietà: per ogni $  a,  b,  c\in V$ e $\lambda\in K$
	\begin{enumerate}
		\item $\intern{  a+  b}{  c}=\intern{  a}{  c}+\intern{  b}{  c}$, e analogamente nella seconda variabile $\intern{  a}{  b+  c}=\intern{  a}{  b}+\intern{  a}{  c}$;
		\item $\intern{\lambda  a}{  c}=\lambda\intern{  a}{  c}$;
		\item $\intern{  a}{  b}=\conj{\intern{  b}{  a}}$;
		\item $\intern{  a}{  a}\geq 0$, ed è nullo se e solo se $  a=0_V$.
	\end{enumerate}
\end{definizione}
Si faccia attenzione particolare alla seconda proprietà: il prodotto interno è lineare solo nella prima variabile, e non lo è nella seconda: se si vuole ``portare fuori'' lo scalare bisogna prendere il suo coniugato, infatti
\begin{equation*}
	\intern{  a}{\lambda  b}=\conj{\intern{\lambda  b}{  a}}=\conj{\lambda\intern{  b}{  a}}=\conj{\lambda}\intern{  a}{  b}.
\end{equation*}
Notiamo inoltre che la quarta proprietà è ben definita, in quanto il prodotto interno di un vettore con sé stesso è sempre un numero reale, dato che $\intern{  x}{  x}=\conj{\intern{  x}{  x}}$ per la terza proprietà, dunque ha senso dire che è maggiore o uguale a zero.
Ecco alcuni esempi di prodotti interni:
\begin{itemize}
	\item In $\R^n$, possiamo prendere $\intern{  x}{  y}=\sum_{i=1}^nx_iy_i$, che è il prodotto scalare citato in precedenza.
		Esso soddisfa in modo ovvio tutte le proprietà date;
	\item In $\C^n$ definiamo \emph{prodotto hermitiano} il prodotto interno $\intern{  x}{  y}=\sum_{i=1}^nx_i\conj{y_i}$, che si indica usualmente con $\hermit{  x}{  y}$;
	\item Possiamo definire anche forme diverse di prodotti interni, come $\intern{  x}{  y}=x_1y_1-x_2y_1-x_1y_2+4x_2y_2$ per $  x,  y\in\R^2$.
		Esso soddisfa le prime tre proprietà, e verifichiamo la quarta: $\intern{  x}{  x}=(x_1-x_2)^2+3x_2^2$, che chiaramente è sempre positivo e nullo solo se $  x=(0,0)^t$.
	\item Nello spazio $\cont{}\ab$ delle funzioni continue definite da un intervallo $[a,b]$ a valori complessi, definiamo
		\begin{equation*}
			\hermit{f}{g}=\int_a^bf(x)\conj{g(x)}\,\dd x.
		\end{equation*}
		Risulta evidente che $\hermit{f}{f}=\int_a^b\abs{f(x)}^2\,\dd x\geq 0$, che inoltre è nullo solo se $f$ è identicamente nulla su $[a,b]$, cioè $f\equiv 0$.
		%Magari si può passare all'integrale di Lebesgue e allo spazio L anziche C?
\end{itemize}

\begin{definizione} \label{d:spazio-euclideo}
	Uno spazio vettoriale $V$ sul campo $K=\R,\C$ dotato di un prodotto interno si chiama \emph{spazio euclideo}.
\end{definizione}
% Ho qualche dubbio su questa definizione: altrove gli spazi euclidei sono gli R^n vari con il prodotto scalare standard. Nel dubbio, non mi riferirò mai a spazi euclidei d'ora in poi, ma specificherò sempre lo spazio e il prodotto interno ogni volta.

Avevamo detto in precedenza che, fissata una delle due variabili, una forma bilineare può essere vista come funzionale lineare.
Con un piccolo aggiustamento, dovuto al fatto che il prodotto interno \emph{non} è lineare nella seconda variabile, possiamo comunque affermare che fissata questa seconda variabile il prodotto interno è a tutti gli effetti un elemento dello spazio duale e risulta un funzionale lineare, quindi può essere rappresentato da una matrice ed esiste il suo inverso (che è detto \emph{antilineare}).% Da ampliare.
\begin{teorema}[di rappresentazione, di Riesz] \label{t:riesz}
	Sia $V$ uno spazio euclideo di dimensione finita con un prodotto interno $\intern{}{}$ non degenere, e $\phi\in V^*$.
	Allora esiste uno ed un solo elemento $  r\in V$ tale per cui $\phi=\intern{\cdot}{  r}$.
\end{teorema}
\begin{proof}
	Dimostriamo dapprima l'esistenza di tale vettore: scelta una base canonica $\{  e_i\}_{i=1}^n$, individuiamo la base $\{\intern{}{  e_i}\}_{i=1}^n$ che è la base dello spazio duale $V^*$, che chiameremo per brevità $\{d_i\}_{i=1}^n$, ed è tale per cui $d_i(  e_j)=\intern{  e_j}{  e_i}=\delta_{ij}$.
	Possiamo definire $\phi$ quindi come combinazione lineare degli elementi di questa base: per opportuni $\lambda_k\in K$ si ha
	\begin{equation*}
		\phi=\sum_{k=1}^n\lambda_kd_k\qqq \phi(  x)=\sum_{k=1}^n\lambda_kd_k(  x).
	\end{equation*}
	Possiamo dunque portare dentro al prodotto interno i coefficienti della combinazione, cioè
	\begin{equation*}
		\phi(  x)=\sum_{k=1}^n\lambda_k\intern{  x}{  e_k}=\intern{  x}{\sum_{k=1}^n\conj{\lambda_k}  e_k}
	\end{equation*}
	e allora il vettore $\sum_{k=1}^n\conj{\lambda_k}  e_k$ è proprio il vettore $  r\in V$ cercato.
	
	Passiamo a dimostrare l'unicità: se esistesse $  r'\in V$ tale che $\phi=\intern{}{  r'}$, allora si dovrebbe avere che $\intern{}{  r}=\intern{}{  r'}$, o anche
	\begin{equation*}
		\intern{  x}{  r}=\intern{  x}{  r'}\ \forall  x\in V.
	\end{equation*}
	Ma allora risulterebbe $\intern{  x}{  r-  r'}=0$: poiché vale per qualsiasi $  x\in V$, vale anche per $  r-  r'$ stesso (che appartiene certamente a $V$), quindi $\intern{  r-  r'}{  r-  r'}=0$ che vale solo se $  r-  r'=0_V$, cioè $  r=  r'$, quindi tale vettore è unico.
\end{proof}
Si noti che $  r$ nella seconda variabile del prodotto interno rende $\phi$ lineare, senza aver bisogno di introdurre i numeri coniugati, poiché nella prima variabile il prodotto interno è lineare. 

\section{Ortogonalità} \label{sec:ortogonalita}
Dalla definizione di prodotto interno possiamo generalizzare a tutti gli spazi il concetto di vettori ortogonali: $  a$ e $  b$ sono ortogonali se il loro prodotto interno è nullo.
Estendiamo la definizione a interi insiemi di vettori.
\begin{definizione} \label{d:insieme-ortogonale}
	Un insieme di vettori $\{  v_i\}_{i\in I}\subset V$ si dice ortogonale se per ogni $j\neq k$ si ha $\intern{  v_j}{  v_k}=0$.
	Inoltre, l'insieme si dice \emph{ortonormale} se $\forall j,k$ si ha $\intern{  v_j}{  v_k}=\delta_{ij}$.
\end{definizione}
Il concetto di ortogonalità (e automaticamente di ortonormalità) è indissolubile dal prodotto interno rispetto a cui la si valuta: un insieme di vettori può ben essere ortogonale rispetto ad un prodotto interno, e non esserlo rispetto ad uno differente. 
La base canonica di $\R^n$ è ortonormale rispetto al prodotto scalare $\intern{  x}{  y}=\sum_{i=1}^nx_iy_i$: si vede facilmente che $x_iy_i=1$ solo se entrambi sono 1, ma questo non accade, a meno ovviamente che $  x=  y$ per cui $x_iy_i=1$ una sola volta da $i=1$ a $i=n$.
Lo stesso accade per la base ortonormale di $\C^n$ con il prodotto hermitiano standard.

Estendiamo ulteriormente il concetto ad interi spazi: sappiamo in $\R^3$ che esistono rette ortogonali anche a un piano, che è non un vettore ma un sottospazio.
Il concetto sottinteso è che la retta è ortogonale al piano perché è ortogonale a tutti le rette contenute nel piano (più rigorosamente, che si intersechino con la prima retta).
\begin{definizione} \label{d:spazio-ortogonale}
	Sia $V$ uno spazio vettoriale su $K$ dotato del prodotto interno $\intern{}{}$ e $S$ un suo sottoinsieme. L'insieme
	\begin{equation*}
		S^\perp=\{  x\in V\colon\forall  s\in S\intern{  x}{  s}=0\}
	\end{equation*}
	è detto insieme ortogonale a $S$, o $S$-ortogonale.
\end{definizione}
Questo insieme è inoltre un sottospazio di $V$: per ogni $  s\in S$ e $  x,  y\in V$, e per ogni $\lambda\in K$ si ha $\intern{  s}{  x+\lambda  y}=\intern{  s}{  x}+\intern{  s}{\lambda  y}=\intern{  s}{  x}+\conj{\lambda}\intern{  s}{  y}=0$.

Gli insiemi ortogonali hanno delle importanti proprietà: innanzitutto, il seguente teorema mostra che sono sempre linearmente indipendenti.
\begin{teorema} \label{t:ortogonale-linearmente-indipendente}
	Siano $V$ uno spazio vettoriale su $K$ dotato del prodotto interno $\intern{}{}$ e $\{  v_i\}_{i\in I}\subset V$ un insieme ortogonale di vettori non nulli.
	Tale insieme è linearmente indipendente.
\end{teorema}
\begin{proof}
	Per ogni sottoinsieme di cardinalità finita $I_0\subset I$, si ha che se $\sum_{i\in I_0}\lambda_i  v_i=0_V$ allora per ogni indice $k\in I_0$ risulterebbe 
	\begin{equation*}
		\intern{\sum_{i\in I_0}\lambda_i  v_i}{  v_k}=0
	\end{equation*}
	perché il primo termine del prodotto è nullo; per linearità portiamo fuori in ciascun addendo $\lambda_i$, ottenendo
	\begin{equation*}
		\sum_{i\in I_0}\lambda_i\intern{  v_i}{  v_k}=0.
	\end{equation*}
	Poiché l'insieme è ortogonale però tutti i prodotti interni sono nulli eccetto $\intern{  v_k}{  v_k}$ e noi abbiamo $\lambda_k\intern{  v_k}{  v_k}=0$.
	Ma $  v_k\neq 0_V$, perché per ipotesi sono tutti non nulli, dunque deve essere $\lambda_k=0$.
	Poiché questo vale $\forall k\in I_0$, la combinazione lineare $\sum_{i\in I_0}\lambda_i  v_i$ è nulla solo quando tutti i $\lambda_i$ sono nulli, quindi l'insieme è linearmente indipendente.
\end{proof}
\begin{corollario} \label{c:ortonormale-linearmente-indipendente}
	Siano $V$ uno spazio vettoriale su $K$ dotato del prodotto interno $\intern{}{}$ e $\{  v_i\}_{i\in I}\subset V$ un insieme ortonormale.
	Tale insieme è linearmente indipendente.
\end{corollario}
La dimostrazione è immediata: un sistema ortonormale non può ammettere elementi nulli, altrimenti non si avrebbe $\intern{  v_i}{  v_i}=1$ per tutti i suoi elementi, quindi ci si riporta immediatamente al teorema precedente.

\begin{teorema}
	Sia $V$ uno spazio vettoriale di dimensione finita con il prodotto interno $\intern{}{}$ e $S=\{  x_i\}_{i=1}^n$ un insieme ortonormale: esso si può completare ad una base ortonormale di $V$.
\end{teorema}
\begin{proof}
	Se $\gen{S}=V$, ne è già una base ortonormale, dunque discutiamo il caso $\gen{S}\neq V$. Deve esistere un $  z\in V\setminus\gen{S}$, e considero il vettore $  e\defeq  z-\sum_{k=1}^n\intern{  z}{  x_k}  x_k$, che non può essere nullo altrimenti sarebbe $  z\in\gen{S}$. Allora
	\begin{equation*}
		\begin{split}
			\intern{  e}{  x_j}=&\intern{  z-\sum_{i=1}^n\intern{  z}{  x_i}  x_i}{  x_j}=\\
				&=\intern{  z}{  x_j}-\sum_{i=1}^n\intern{  z}{  x_k}\intern{  x_k}{  x_j}=\\
				&=\intern{  z}{  x_j}-\sum_{i=1}^n\intern{  z}{  x_k}\delta_{kj}=\\
				&=\intern{  z}{  x_j}-\intern{  z}{  x_j}=0
		\end{split}
	\end{equation*}
	quindi $  e$ e $  x_j$ sono ortogonali, per ogni $j\in\{1,\dots,n\}$.
	Definisco allora
	\begin{equation*}
		  x_{n+1}\defeq \frac{  e}{\sqrt{\intern{  e}{  e}}}.
	\end{equation*}
	L'insieme $\{  x_i\}_{i=1}^{n+1}$ è ortonormale, e per il corollario precedente anche linearmente indipendente.
	Se esso non genera ancora $V$, ripetiamo il procedimento aggiungendo altri elementi all'insieme.
	Poiché la dimensione di $V$ è finita, le sue basi hanno un numero finito $m$ di elementi, quindi il procedimento deve avere una fine, arrivando all'insieme $\{  x_i\}_{i=1}^{n+r}$, con $n+r=m$, che è sempre ortonormale e linearmente indipendente; avendo inoltre $m$ elementi, per il teorema \ref{t:base-dimensione} esso è una base di $V$.
\end{proof}
\begin{teorema}
	Sia $V$ uno spazio euclideo di dimensione $k<\pinf$ con il prodotto interno $\intern{}{}$ e $\{  v_i\}_{i=1}^k$ un insieme ortonormale.
	Le due seguenti affermazioni sono equivalenti:
	\begin{enumerate}
		\item $\{  v_i\}_{i=1}^k$ è una base di $V$;
		\item se $\intern{  z}{  v_j}=0$ per ogni $j\in\{1,\dots,k\}$ allora $  z=0_V$.
	\end{enumerate}
\end{teorema}
\begin{proof}
	$(1\then 2)$ Per assurdo, esista un $  z\neq 0_V$ per cui $\intern{  z}{  v_j}=0$ per ogni $j\in\{1,\dots,k\}$.
	Prendo $  e\defeq  z-\sum_{i=1}^k\intern{  z}{  v_i}  v_i$: esso è tale che $\intern{  e}{  v_j}=0$.
	L'insieme $\{  v_i,\frac1{\sqrt{\intern{  e}{  e}}}\}_{i=1}^k$ è ancora ortonormale per il teorema precedente.
	Questo insieme però ha più elementi della dimensione $k$ dello spazio, dunque non può essere linearmente indipendente per il teorema \ref{t:base-dimensione} e quindi non è nemmeno una base.
	
	$(2\then 1)$ Se $\gen{\{  v_i\}_{i=1}^k}\neq V$, per il teorema precedente si troverebbe un elemento $  e$ non nullo tale che $\{  e,  v_i\}_{i=1}^k$ è ortonormale, ossia $\intern{  e}{  e_j}=0$ $\forall j\in\{1,\dots,k\}$, che contraddice l'ipotesi.
	Allora deve essere $\gen{\{  v_i\}_{i=1}^k} = V$, e dato che è ortonormale è anche linearmente indipendente per il teorema \ref{c:ortonormale-linearmente-indipendente}, quindi è una base di $V$.
\end{proof}

\section{Spazi normati} \label{sec:spazi-normati}
Definiamo innanzizutto la norma, una particolare applicazione dallo spazio $V$ al campo dei numeri reali.
\begin{definizione} \label{d:norma}
	Dato uno spazio vettoriale $V$ sul campo $K$, si definisce \emph{norma} l'applicazione $\norm{\cdot}\colon V\to\R$ che soddisfa le seguenti proprietà, per ogni $  x,  y\in V$:
	\begin{enumerate}
		\item $\norm{  x}\geq 0$;
		\item $\norm{  x}=0$ se e solo se $  x=0_V$;
		\item $\norm{\lambda  x}=\abs{\lambda}\norm{  x}$ per ogni $\lambda\in K$;
		\item $\norm{  x+  y}\leq\norm{  x}+\norm{  y}$.
	\end{enumerate}
\end{definizione}
Uno spazio vettoriale in cui è definita una norma si chiama \emph{spazio normato}.
Da un prodotto interno si può sempre definire una norma, con
\begin{equation*}
	\norm{  x}=\sqrt{\intern{  x}{  x}}.
\end{equation*}
Le prime due proprietà sono automaticamente soddisfatte dalle proprietà della radice quadrata e del prodotto interno. Inoltre
\begin{equation*}
	\begin{split}
		\norm{\lambda  x}&=\sqrt{\intern{\lambda  x}{\lambda  x}}=\sqrt{\lambda\conj{\lambda}\intern{  x}{  x}}=\\
			 &=\sqrt{\abs{\lambda}^2\intern{  x}{  x}}=\abs{\lambda}\sqrt{\intern{  x}{  x}}=\abs{\lambda}\norm{  x}.
	\end{split}
\end{equation*}
La quarta proprietà discende dalla disuguaglianza di Cauchy-Schwarz, $\abs{\intern{  x}{  y}}\leq\norm{  x}\norm{  y}$.
Chiamiamo $\alpha\defeq\intern{  y}{  y}$ e $\beta\defeq-\intern{  x}{  y}$.
Il prodotto interno di $\alpha  x+\beta  y$ con sé stesso è sempre positivo, dunque scomponendolo otteniamo
\begin{equation*}
	\begin{split}
		\intern{\alpha  x+\beta  y}{\alpha  x+\beta  y}=\\
		=&\intern{\alpha  x}{\alpha  x}+\intern{\beta  y}{\alpha  x}+\intern{\alpha  x}{\beta  y}+\intern{\beta  y}{\beta  y}=\\
		=&\alpha\conj{\alpha}\intern{  x}{  x}+\beta\conj{\alpha}\intern{  y}{  x}+\alpha\conj{\beta}\intern{  x}{  y}+\beta\conj{\beta}\intern{  y}{  y}\geq 0.
	\end{split}
\end{equation*}
Poiché $\alpha=\intern{  y}{  y}=\norm{  y}^2$, quindi è un numero reale, sostituendo i valori di $\alpha$ e $\beta$ abbiamo
\begin{equation*}
	\norm{  y}^4\norm{  x}^2-2\norm{  y}^2\intern{  x}{  y}\conj{\intern{  x}{  y}}+\norm{  y}^2\intern{  x}{  y}\conj{\intern{  x}{  y}}\geq 0.
\end{equation*}
Poiché $\conj{\intern{  x}{  y}}\intern{  x}{  y}=\abs{\intern{  x}{  y}}^2$ abbiamo che
\begin{equation*}
	\norm{  y}^2\abs{\intern{  x}{  y}}^2\leq\norm{  x}^2\norm{  y}^4,
\end{equation*}
Se $\norm{  y}=0$ allora $  y=0_V$ e la disuguaglianza è ovvia, altrimenti la disuguaglianza di Cauchy si ottiene dividendo per $\norm{  y}^2$ e prendendo le radici quadrate dei due membri.

Passiamo ora a verificare la quarta proprietà della norma dal prodotto interno usando questo risultato.
Abbiamo che $\norm{  x+  y}^2=\intern{  x+  y}{  x+  y}=\intern{  x}{  x}+\intern{  x}{  y}+\intern{  y}{  x}+\intern{  y}{  y}$, ma $\intern{  x}{  y}+\intern{  y}{  x}=\intern{  x}{  y}+\conj{\intern{  x}{  y}}\leq 2\abs{\intern{  x}{  y}}$, quindi dalla disuguaglianza di Cauchy-Schwarz\footnote{Ricordiamo che per $z\in\C$ si ha $z+\conj{z}\leq2\abs{z}$. Infatti sia $z=\rho\cos\theta+i\rho\sin\theta$ in forma trigonometrica: allora $z+\conj{z}=\rho\cos\theta+i\sin\theta+\rho\cos\theta-i\sin\theta=2\rho\cos\theta\leq 2\rho=2\abs{z}$. }
\begin{equation*}
	\norm{  x+  y}^2\leq \norm{  x}^2+2\abs{\intern{  x}{  y}}+\norm{  y}^2
	\leq\norm{  x}^2+2\norm{  x}\norm{  y}+\norm{  y}^2=(\norm{  x}+\norm{  y})^2.
\end{equation*}
Poiché entrambi i membri sono positivi, basta prendere la radice quadrata di entrambi per verificare la proprietà.

Ecco un esempio di norma che non deriva da un prodotto interno: prendiamo lo spazio $\ell^1(\R)$ delle successioni di numeri reali sommabili, ossia
\begin{equation*}
	\ell^1(\R)=\bigg\{\{a_n\}_{n\in\N}\subset\R\colon\ser{i}\abs{a_n}<\pinf\bigg\}.
\end{equation*}
In esso si può definire la norma $\norm{\{a_n\}}\defeq\ser{i}\abs{a_n}$.
Ovviamente è sempre positiva, ed è nulla solo se $a_n=0$ $\forall n\in N$.
Verifichiamo la terza proprietà: $\lambda\{a_n\}=\{\lambda a_n\}$, quindi
\begin{equation*}
	\norm{\lambda\{a_n\}}=\ser{i}\abs{\lambda a_n}=\abs{\lambda}\ser{i}\abs{a_n}=\lambda\norm{\{a_n\}}.
\end{equation*}
La quarta:
\begin{equation*}
	\norm{\{a_n\}+\{b_n\}}=\ser{i}\abs{a_n+b_n}\leq\ser{i}(\abs{a_n}+\abs{b_n})
	=\ser{i}\abs{a_n}+\ser{i}\abs{b_n}=\norm{\{a_n\}}+\norm{\{b_n\}},
\end{equation*}
in cui abbiamo potuto separare la serie di $(\abs{a_n}+\abs{b_n})$ nella somma delle due serie poiché sono assolutamente convergenti.

\begin{proprieta} \label{p:regola-parallelogramma}
	Sia $V$ uno spazio normato con il prodotto interno $\intern{}{}$.
	La sua norma $\norm{}$ proviene da un prodotto interno se e solo se rispetta l'equazione, per ogni $  x,  y\in V$,
	\begin{equation}
		\norm{  x+  y}^2+\norm{  x-  y}^2=2(\norm{  x}^2+\norm{  y}^2).
		\label{eq:regola-parallelogramma}
	\end{equation}
\end{proprieta}
\begin{proof}
	Scrivendo la norma dei vettori come il loro prodotto interno con se stessi, abbiamo che
	\begin{equation*}
		\begin{split}
			\norm{  x+  y}^2+\norm{  x-  y}^2&=\intern{  x+  y}{  x+  y}+\intern{  x-  y}{  x-  y}=\\
			&=\intern{  x}{  x+  y}+\intern{  y}{  x+  y}+\intern{  x}{  x-  y}-\intern{  y}{  x-  y}=\\
			&=\intern{  x}{  x}+\intern{  x}{  y}+\intern{  y}{  x}+\intern{  y}{  y}+\intern{  x}{  x}-\intern{  x}{  y}-\intern{  y}{  x}+\intern{  y}{  y}=\\
			&=2\intern{  x}{  x}+2\intern{  y}{  y}=\\
			&=2\norm{  x}^2+2\norm{  y}^2.
		\end{split}
	\end{equation*}
\end{proof}

\begin{proprieta} \label{p:formule-polarizzazione}
	Sia $V$ uno spazio normato dal prodotto interno $\intern{}{}$.
	Allora per ogni $  x,  y\in V$ si ha
	\begin{gather}
		\intern{  x}{  y}=\frac14\norm{  x+  y}^2-\frac14\norm{  x-  y}^2\text{ se il campo è }\R
		\label{eq:polarizzazione-R}\\
		\intern{  x}{  y}=\frac14\norm{  x+  y}^2-\frac14\norm{  x-  y}^2+\frac{i}4\norm{  x+i  y}^2-\frac{i}4\norm{  x-i  y}^2\text{ se il campo è }\C
		\label{eq:polarizzazione-C}
	\end{gather}
\end{proprieta}
\begin{proof}
	Dimostriamo la prima formula, esprimendo la norma della somma e della sottrazione in termini del prodotto interno.
	\begin{multline*}
		\norm{  x+  y}^2=\intern{  x+  y}{  x+  y}=\intern{  x}{  x+  y}+\intern{  y}{  x+  y}=\\
		=\intern{  x}{  x}+\intern{  y}{  y}+2\intern{  x}{  y}=\norm{  x^2}+\norm{  y}^2+2\intern{  x}{  y}
	\end{multline*}
	e allo stesso modo si ha $\norm{  x+  y}^2=\norm{  x}^2+\norm{  y}^2-2\intern{  x}{  y}$.
	Sottraendole si ottiene $\norm{  x+  y}^2-\norm{  x-  y}^2=4\intern{  x}{  y}$ da cui la tesi. Il caso due non è invece qui dimostrato.
\end{proof}
Ad esempio nello spazio $\cont{}\ab$ di funzioni a valori complessi possiamo definire la norma
\begin{equation*}
	\norm{f}=\bigg(\int_a^b\abs{f(x)}^2\,\dd x\bigg)^2,
\end{equation*}
che deriva dal prodotto hermitiano che avevamo definito precedentemente.

\begin{teorema} \label{t:identita-parseval-bessel}
	Sia $V$ uno spazio dotato del prodotto interno $\intern{}{}$ e di dimensione finita, e sia $\{  e_i\}_{i=1}^n$ un insieme ortonormale.
	Le seguenti affermazioni sono equivalenti:
	\begin{enumerate}
		\item $\{  e_i\}_{i=1}^n$ è una base di $V$;
		\item $\forall  x\in V$ si ha $  x=\sum_{i=1}^n\intern{  x}{  e_i}  e_i$;
		\item $\forall  x,  y\in V$ si ha $\intern{  x}{  y}=\sum_{i=1}^n\intern{  x}{  e_i}\conj{\intern{  y}{  e_i}}$, nota come \emph{identità di Parseval};
		\item $\forall  x\in V$ si ha $\norm{  x}^2=\sum_{i=1}^n\abs{\intern{  x}{  e_i}}^2$, nota come \emph{identità di Bessel}.
	\end{enumerate}
\end{teorema}
\begin{proof}
	$(1\then 2)$ $\{  e_i\}_{i=1}^n$ è una base quindi possiamo sempre scrivere $  x$ come combinazione lineare dei suoi elementi, cioè $  x=\sum_{j=1}^n\lambda_j  e_j$.
	Se prendiamo poi il prodotto interno di $  x$ con un elemento della base otteniamo
	\begin{equation}
		\intern{  x}{  e_k}=\intern{\sum_{j=1}^n\lambda_j  e_j}{  e_k}=\sum_{j=1}^n\lambda_k\intern{  e_j}{  e_k}=\sum_{j=1}^n\lambda_k\delta_{jk}=\lambda_k
	\end{equation}
	quindi i coefficienti sono dati dal prodotto interno con ciascun elemento della base, cioè $  x=\sum_{k=1}^n\intern{  x}{  e_k}  e_k$.

	$(2\then 3)$ Esprimendo $  x$ e $  y$ come ricavato nel punto precedente abbiamo
	\begin{equation}
		\begin{split}
			\intern{  x}{  y}&=\intern{\sum_{i=1}^n\intern{  x}{  e_i}  e_i}{\sum_{j=1}^n\intern{  y}{  e_j}  e_j}=\\
			&=\sum_{i=1}^n\intern{  x}{  e_i}\conj{\sum_{j=1}^n\intern{  y}{  e_j}}\intern{  e_i}{  e_j}=\\
			&=\sum_{i=1}^n\intern{  x}{  e_i}\conj{\sum_{j=1}^n\intern{  y}{  e_j}}\delta_{ij}=\\
			&=\sum_{i=1}^n\intern{  x}{  e_i}\conj{\intern{  y}{  e_i}}.
		\end{split}
	\end{equation}
	
	$(3\then 4)$ Basta sostituire $  x$ a $  y$ per ottenere l'identità di Bessel in modo ovvio, dato che $\norm{  x}^2=\intern{  x}{  x}$.

	$(4\then 1)$ Preso un qualunque $  z\in V$, per il punto 2 possiamo esprimerlo come $  z=\sum_{i=1}^n\intern{  z}{  e_i}  e_i$.
	L'insieme $\{  e_i\}_{i=1}^n$ è linearmente indipendente: la combinazione lineare dei suoi elementi, che dà $  z$, non può mai dare il vettore nullo.
	Infatti se $  z=0_V$ allora ovviamente $\intern{  z}{  e_j}=0$ per ogni $j\in\{1,\dots,n\}$, e viceversa se $\intern{  z}{  e_k}=0$ $\forall k\in\{1,\dots,n\}$ allora per l'identità di Bessel $\norm{  z}^2=\sum_{k=1}^n\abs{\intern{  z}{  e_k}}^2=0$ quindi $  z=0_V$.
	Allora l'insieme degli $  e_i$ è linearmente indipendente e ha cardinalità $n=\dim V$, perciò ne è una base.
\end{proof}

\section{Operatori aggiunti}
\begin{definizione} \label{d:aggiunto}
	Sia $V$ uno spazio vettoriale dotato del prodotto interno $\intern{}{}$, e $T$ un endomorfismo in $V$.
	Si definisce \emph{aggiunto} di $T$, se esiste, l'operatore $\adj T$ (anch'esso un endomorfismo di $V$) tale per cui per ogni coppia $  x,  y\in V$ risulta
	\begin{equation*}
		\intern{T(  x)}{  y}=\intern{  x}{\adj T(  y)}.
	\end{equation*}
\end{definizione}
\begin{teorema} \label{t:unicita-aggiunto}
	Se $V$ è uno spazio vettoriale di dimensione finita dotato del prodotto interno $\intern{}{}$, e $T$ un endomorfismo in esso, allore esiste un unico endomorfismo che sia l'aggiunto di $T$.
\end{teorema}
\begin{proof}
	Sia $S\in\End(V)$ l'aggiunto di $T$, cioè sia $\intern{T(  x)}{  y}=\intern{  x}{S(  y)}$ per ogni $  x,  y\in V$.
	Allora
	\begin{equation}
		0=\intern{T(  x)}{  y}-\intern{T(  x)}{  y}=\intern{  x}{\adj{T}(  y)}-\intern{  x}{S(  y)}.
	\end{equation}
	Poich\'e vale per ogni $  x$, scegliamo $  x=\adj{T}(  y)-S(  y)$ ottenendo $\norm{\adj{T}(  y)-S(  y)}^2=0$ da cui $\adj{T}(  y)=S(  y)$.
	L'uguaglianza vale per ogni $  y\in V$, quindi i due endomorfismi devono coincidere.
\end{proof}
\begin{teorema} \label{t:matrice-aggiunta}
	Rispetto a una base ortonormale, la matrice associata all'aggiunto di $T$ è la trasposta coniugata della matrice associata a $T$, cioè se $A$ è la matrice associata a $T$ si avrà $\conj{A^t} = A^*$.
\end{teorema}
\begin{proof}
	% Inserire la dimostrazione che l'aggiunto è lineare?
	Sia $\{  e_i\}_{i=1}^n$, con $n=\dim V$, una base ortonormale di $V$.
	Sia $A$ la matrice associata a $T$ in tale base, e $B$ quella associata a $\adj T$; indicheremo la matrice trasposta coniugata con il simbolo $\adj{A}$, come per l'endomorfismo.
	La $j$-esima colonna di $A$ è $T(  e_i)=\sum_{j=1}^nA_{ji}  e_j$, quindi
	\begin{equation}
		\intern{T(  e_i)}{  e_k}=\intern{\sum_{j=1}^nA_{ji}  e_j}{  e_k}=\sum_{j=1}^nA_{ji}\intern{  e_j}{  e_k}=\sum_{j=1}^nA_{ji}\delta_{jk}=A_{ki}.
	\end{equation}
	Analogamente si prova che $B_{ki}=\intern{\adj T(  e_i)}{  e_k}$.
	Ma allora possiamo scrivere le uguaglianze
	 	\[
	 	B_{ki}=\intern{\adj T(  e_i)}{  e_k}=\conj{\intern{  e_k}{\adj T(  e_i)}}=\conj{\intern{T(  e_k)}{  e_i}}=\conj A_{ik}=\conj{A^t}_{ki}.
	 	\]
	quindi $B=\conj{A^t}=\adj A$.
\end{proof}
Per le matrici reali, la trasposta coniugata si riduce alla sola trasposta.

\begin{proprieta} \label{p:proprieta-aggiunto}
	Sia $V$ uno spazio vettoriale di dimensione finita con il prodotto interno $\intern{}{}$.
	Per $T,S\in\End(V)$ e $\lambda\in K$ si hanno le seguenti proprietà:
	\begin{enumerate}
		\item $\adj{(T+S)}=\adj T+\adj S$;
		\item $\adj{(\lambda T)}=\conj{\lambda}\adj T$;
		\item $\adj{(T\circ S)}=\adj S\circ\adj T$;
		\item $\adj{(\adj T)}=T$;
		\item se $T$ è inoltre invertibile, $\adj{(T^{-1})}=(\adj T)^{-1}$.
	\end{enumerate}
\end{proprieta}
\begin{proof}
	La prima discende dalla linearità nella prima variabile del prodotto interno, basta considerare
	\[\begin{split}
		\intern{(T+S)(  x)}{  y} &= \intern{T(  x)+S(  x)}{  y} = \intern{T(  x}{  y} + \intern{S(  x)}{  y} =\\ &= \intern{  x}{\adj T(  y)} + \intern{  x}{\adj S(  y)} = \intern{  x}{\adj T(  x) + \adj S(  x)} = \intern{  x}{(\adj T + \adj S)(  x)},
	\end{split}\]
	in cui i passaggi sono stati possibili per la  linearità dell'applicazione e per le proprietà dell'aggiunto.
	Quindi $(\adj T + \adj S) = \adj{(T + S)}$.
	
	La seconda si ricava dall'antilinearità nella seconda variabile sempre del prodotto interno, per vederlo è sufficiente analizzare
	\[
		\intern{\lambda T(  x)}{  y} = \lambda \intern{T(  x)}{  y} = \lambda \intern{  x }{\adj T(  y)} = \intern{  x}{ \conj{\lambda}\adj T(  y)},
	\]
	Si ricava allora che deve essere $\adj T \conj{\lambda} = \adj{( T \lambda)}$.
	
	Per la terza proprietà abbiamo per ogni $  x,  y\in V$
	\begin{equation}
		\intern{(T\circ S)(  x)}{  y}=\intern{T\big(S(  x)\big)}{  y}=\intern{S(  x)}{\adj T(  y)}=\intern{  x}{\adj S\big(\adj T(  y)\big)}.
	\end{equation}
	Dimostriamo poi la quarta:
	\begin{equation}
		\intern{\adj T(  x)}{  y}=\conj{\intern{  y}{\adj T(  x)}}=\conj{\intern{T(  y)}{  x}}=\intern{  x}{T(  y)},
	\end{equation}
	cioè l'aggiunto di $\adj T$ è $T$ stesso.
\end{proof}

Riprendiamo ora il concetto di sottospazio invariante introdotto nella sezione \ref{sec:t-invariante}.
\begin{teorema} \label{t:invariante-ortogonale}
	Sia $V$ uno spazio vettoriale di dimensione finita dotato del prodotto interno $\intern{}{}$, e $T\in\End(V)$.
	Un sottospazio $W\leq V$ è $T$-invariante se e solo se $W^\perp$ è $\adj T$-invariante.
\end{teorema}
\begin{proof}
	Per ogni $  w\in W$ e $  x\in W^\perp$ si ha
	\begin{equation}
		\intern{\adj T(  x)}{  w}=\conj{\intern{  w}{\adj T(  x)}}=\conj{\intern{T(  w)}{  x}}=0,
	\end{equation}
	dato che $T(  w)\in W$, poich\'e $W$ è $T$-invariante, e $  x\in W^\perp$.
	Allora ogni $  x\in W^\perp$ è tale che $\adj T(  x)$ appartiene ancora a $W^\perp$, cioè $W^\perp$ è $\adj T$-invariante.

	Sia ora $W^\perp$ $\adj T$-invariante, allora $(W^\perp)^\perp$ deve essere $\adj{(\adj T)}$-invariante, cioè $W$ è $T$-invariante.
\end{proof}
Prendiamo ancora un sottospazio $W\leq V$ che sia invariante rispetto a $T\in\End(V)$: possiamo restringere $T$ al sottospazi ottenendo $T|_W$, che porta elementi di $W$ ancora in elementi di $W$, dunque $T|_W\in\End(W)$.
Allo stesso modo, per il teorema appena enunciato, $\adj T|_{W^\perp}\in\End(W^\perp)$.
Si faccia attenzione però che scrivere $\adj{(T|_W)}=\adj T|_{W^\perp}$ non ha alcun senso: l'uguaglianza non può essere vera, poich\'e si ha che $W\cap W^\perp=\{0_W\}$, quindi si uguaglierebbero due applicazioni definite su spazi che non hanno elementi in comune (a parte lo zero)!
\footnote{Quest'ultimo fatto si nota subito dal fatto che preso $  z\in W\cap W^\perp$, esso non può che essere lo zero poich\'e deve risultare $\intern{  z}{  z}=0$, cioè $\norm{  z}=0$.}

\section{Operatori normali}
\begin{definizione} \label{d:operatore-normale}
	Sia $V$ uno spazio vettoriale con il prodotto interno $\intern{}{}$.
	Un endomorfismo $T\in\End(V)$ si dice \emph{normale} se commuta con il suo aggiunto, ossia se
	\begin{equation*}
		T\circ\adj T=\adj T\circ T.
	\end{equation*}
\end{definizione}
La definizione vale sia per gli spazi di dimensione finita che infinita: nei primi l'aggiunto esiste sempre, come abbiamo dimostrato in precedenza, mentre negli altri l'esistenza non è certa, quindi bisogna prima verificare anche questo fatto.
Data una base ortonormale, se $A$ è la matrice associata a $T$ e $\adj A$ a $\adj T$, si ha che
\begin{equation*}
	A\adj A=\adj AA
\end{equation*}
quindi anche la matrice associata commuta con la sua trasposta coniugata: le matrici aventi questa proprietà vengono coerentemente chiamate \emph{matrici normali}.
% Inserire alcuni esempi di classi di matrici che sono normali.

si può anche definire l'endomorfismo normale a partire dalla sua matrice associata: un endomorfismo è normale se la matrice ad esso associata in una base ortonormale è una matrice normale.
Questa definizione è corretta, in quanto se la matrice è normale rispetto ad una particolare base ortonormale allora lo è anche rispetto a qualsiasi altra base ortonormale.
\begin{proprieta}
	Se $V$ è uno spazio vettoriale normato e $T\in\End(V)$ è normale, allora $\forall  x\in V$ si ha $\norm{T(  x)}=\norm{\adj T(  x)}$.
\end{proprieta}
\begin{proof}
Passando al prodotto interno, il quadrato della norma di $T(  x)$ è
\begin{multline*}
	\norm{T(  x)}^2=\intern{T(  x)}{T(  x)}=\intern{  x}{\adj T\big(T(  x)\big)}=\intern{  x}{T\big(\adj T(  x)\big)}=\\
	=\conj{\intern{T\big(\adj T(  x)\big)}{  x}}=\conj{\intern{\adj T(  x)}{\adj T(  x)}}=\conj{\norm{\adj T(  x)}^2}=\norm{\adj T(  x)}^2
\end{multline*}
poich\'e la norma è un numero reale, dunque lo è anche il suo quadrato che coincide con il coniugato. 
\end{proof}
\begin{teorema} \label{t:autovalori-operatore-normale}
	Sia $V$ uno spazio vettoriale dotato del prodotto interno $\intern{}{}$ e di dimensione finita, e $T\in\End(V)$ normale.
	Un vettore $  v\in V$ è un autovettore di $T$ relativo all'autovalore $\lambda$ se e solo se è anche autovettore di $\adj T$ relativo all'autovalore $\conj{\lambda}$.
	Inoltre, gli autospazi relativi ai due autovalori $\lambda$ (per $T$) e $\conj\lambda$ (per $\adj T$) coincidono.
\end{teorema}
\begin{proof}
	Poich\'e $  v$ è un autovettore relativo all'autovalore $\lambda$, si deve avere $(T-\lambda I)  v=0_V$, quindi $\norm{(T-\lambda I)  v}=0$.
	Inoltre il suo aggiunto è $\adj{(T-\lambda I)}=\adj T-\adj{\lambda I}=\adj T-\conj\lambda I$. Allora
	\begin{equation}
		(T-\lambda I)\circ\adj{(T-\lambda I)}=(T-\lambda I)\circ(\adj T-\conj\lambda I)=T\adj T-\lambda\adj T-\conj\lambda T+\abs{\lambda}^2 I	
	\end{equation}
	mentre scambiando l'ordine si ha
	\begin{equation}
		\adj{(T-\lambda I)}\circ(T-\lambda I)=\adj TT-\conj\lambda T-\lambda\adj T+\abs{\lambda}^2 I,
	\end{equation}
	che è lo stesso risultato poich\'e $\adj TT=T\adj T$ dato che $T$ è normale: quindi anche $T-\lambda I$ è normale.
	Per la proprietà precedente, allora, risulta
	\begin{equation}
		0=\norm{(T-\lambda I)  v}=\norm{\adj{(T-\lambda I)}  v}=\norm{(\adj T-\conj\lambda I)  v},
	\end{equation}
	cioè $  v$ è anche autovettore di $\adj T$ relativo all'autovalore $\conj\lambda$.
	L'implicazione inversa è dimostrata ovviamente allo stesso modo.
\end{proof}

\begin{proprieta} \label{pr:autospazi-ortogonali}
	Sia $V$ uno spazio vettoriale di dimensione finita dotato del prodotto interno $\intern{}{}$.
	Se $T$ è un endomorfismo normale di $V$ e $\lambda,\mu$ sono due suoi autovalori distinti, allora gli autospazi ad essi associati sono ortogonali.
\end{proprieta}
\begin{proof}
	Siano $V_\lambda$ e $V_\mu$ gli autospazi relativi ai due autovalori $\lambda$ e $\mu$, distinti, e siano $  v\in V_\lambda$ e $  w\in V_\mu$.
	Si ha che
	\begin{equation}
		\lambda\intern{  v}{  w}=\intern{\lambda  v}{  w}=\intern{T(  v)}{  w}=\intern{  v}{\adj T(  w)}=\intern{  v}{\conj{\mu}  w}=\mu\intern{  v}{  w}.
	\end{equation}
	Sottraendo il primo all'ultimo si ottiene che $(\lambda-\mu)\intern{  v}{  w}=0$.
	Dato che i due autovalori sono distinti, ciò può accadere soltanto se $\intern{  v}{  w}=0$, quindi $  v\perp   w$.
	Poich\'e questo accade per ogni $  v\in V_\lambda$ e $  w\in V_\mu$, allora $V_\lambda\perp V_\mu$.
\end{proof}

La proprietà principale delle matrici normali è che sono \emph{sempre diagonalizzabili}, dunque in uno spazio vettoriale complesso si può sempre trovare una base ortonormale che è costituita da autovettori di un endomorfismo normale.
Questo risultato prende il nome di \emph{teorema spettrale}.
\begin{teorema}[Teorema spettrale] \label{t:spettrale}
	Sia $V$ uno spazio vettoriale su $\C$ di dimensione finita, dotato del prodotto interno $\intern{}{}$, e $T$ un endomorfismo normale di $V$.
	$T$ è sempre diagonalizzabile.
\end{teorema}
\begin{proof}
	Poiché il campo dello spazio è il campo dei numeri complessi, il polinomio caratteristico ammette sempre almeno una radice (per il teorema fondamentale dell'algebra \ref{t:fondamentale-algebra}).
	Sia $\lambda_1$ tale radice: se $V_{\lambda_1}$ coincide con $V$, il teorema è subito dimostrato.
	Si assuma quindi $V_{\lambda_1}\neq V$.
	Poiché $T$ è normale, l'autospazio relativo a $\lambda_1$ coincide con quello relativo al suo coniugato $\conj{\lambda_1}$, perché quest'ultimo è un autovalore dell'aggiunto $\adj T$, per il teorema \ref{t:autovalori-operatore-normale}.
	Allora dato che $\lambda_1$ è un autovalore di $T$, $V_{\lambda_1}$ è $T$-invariante per il lemma \ref{l:t-invarianza-autovalori}, quindi lo è anche $V_{\conj{\lambda_1}}
	$, e analogamente entrambi sono anche $\adj T$-invarianti perch\'e $\conj\lambda$ è un autovalore di $\adj T$.
	Inoltre $V_{\lambda_1}^\perp$ è $\adj T$-invariante per il teorema \ref{t:invariante-ortogonale}.
	Ma $V_{\lambda_1}^\perp\equiv V_{\conj{\lambda_1}}^\perp$, quindi con lo stesso ragionamento di prima $V_{\lambda_1}^\perp$ è $\adj{(\adj T)}$-invariante, cioè $T$-invariante.

	Si restringa dunque l'operatore $T$ al sottospazio $V_{\lambda_1}^\perp$. L'applicazione $T|_{V_{\lambda_1}^\perp}$ è un endomorfismo sul sottospazio $V_{\lambda_1}^\perp$, ed è ancora normale.
	Questo sottospazio è ovviamente ancora costruito sul campo complesso e ha una dimensione finita non nulla, quindi $T|_{V_{\lambda_1}^\perp}$ ha sempre un autovalore, che indichiamo con $\lambda_2$, allora esiste $  w\in V_{\lambda_1}^\perp$, non nullo, tale che $T(  w)=\lambda_2  w$.
	Necessariamente si ha $\lambda_1\neq\lambda_2$, perché altrimenti $  w$ apparterrebbe anche a $V_{\lambda_1}$, ma l'intersezione tra $V_{\lambda_1}$ e $V_{\lambda_1}^\perp$ è il solo zero.
	Se ora $V_{\lambda_2}\equiv V_{\lambda_1}^\perp$, si ha che $V_{\lambda_2}\oplus V_{\lambda_1}=V_{\lambda_1}\oplus V_{\lambda_1}^\perp=V$ quindi il teorema è dimostrato.
	Se invece $V_{\lambda_2}$ non coincide con $V_{\lambda_1}^\perp$, si ha almeno che $V_{\lambda_2}\oplus V_{\lambda_1}$ (che questa volta non coincide con tutto $V$) è $\adj T$-invariante, quindi il suo ortogonale $(V_{\lambda_2}\oplus V_{\lambda_1})^\perp$ è $T$-invariante.
	Dunque, ancora una volta, $T$ ristretto a questo sottospazio è un endomorfismo normale, perciò esiste $  z\in(V_{\lambda_2}\oplus V_{\lambda_1})^\perp$ non nullo tale che $T(  z)=\lambda_3  z$, con $\lambda_3$ diverso da $\lambda_1$ e $\lambda_2$.

	Procedendo in questo modo, ogni volta restringendo l'insieme di definizione di $T$, si giunge necessariamente con un numero finito di passi ad una conclusione in cui la somma diretta degli autospazi forma tutto $V$, poiché la sua dimensione è finita.
	Dunque $T$ è diagonalizzabile.
\end{proof}

\section{Operatori autoaggiunti} \label{sec:operatori-autoaggiunti}
\begin{definizione} \label{d:operatore-autoaggiunto}
	Sia $V$ uno spazio vettoriale dotato di un prodotto interno.
	Un endomorfismo si dice \emph{autoaggiunto} se coincide con il suo aggiunto.
\end{definizione}
Affinch\'e un operatore possa essere autoaggiunto, deve prima esistere il suo aggiunto; come già visto, in spazi di dimensione finita questo è garantito.
Un operatore autoaggiunto è ovviamente anche normale, poich\'e commuta con s\'e stesso.
La matrice associata, in una base ortonormale, ad un operatore autoaggiunto è una matrice hermitiana, che coincide con la sua trasposta coniugata.
\begin{teorema}	\label{t:autovalori-operatore-autoaggiunto}
	Sia $V$ uno spazio vettoriale con il prodotto interno $\intern{}{}$.
	Se $T\in\End(V)$ è autoaggiunto, tutti i suoi autovalori sono reali.
\end{teorema}
\begin{proof}
	Se $V$ è uno spazio sul campo dei numeri reali, la tesi è ovviamente già dimostrata, quindi prendiamo $V$ sul campo complesso.
	Sia $\lambda$ un autovalore di $T$ e $  x$ un autovettore associato: allora
	\begin{equation}
		\lambda\intern{  x}{  x}=\intern{\lambda  x}{  x}=\intern{T(  x)}{  x}=\intern{  x}{\adj T(  x)}=\intern{  x}{T(  x)}=\intern{  x}{\lambda  x}=\conj\lambda\intern{  x}{  x}.
	\end{equation}
	Poich\'e $  x$ è un autovettore non può essere nullo perciò $\intern{  x}{  x}\neq 0$, quindi deve essere $\lambda=\conj\lambda$ cioè $\lambda\in\R$.
\end{proof}

\begin{teorema}
	Sia $V$ uno spazio vettoriale di dimensione finita, dotato di prodotto interno.
	Se $T\in\End(V)$ autoaggiunto, un sottospazio $W\leq V$ è $T$-invariante se e solo se lo è anche $W^\perp$.
\end{teorema}
\begin{proof}
	Segue immediatamente dal teorema \ref{t:invariante-ortogonale}, in cui poniamo $T=\adj T$.
\end{proof}

\begin{teorema}
	Sia $V$ uno spazio vettoriale dotato di un prodotto interno.
	Se $T\in\End(V)$ è autoaggiunto, allora ammette sempre almeno un autovalore.
\end{teorema}
\begin{proof}
	Se $V$ è sul campo complesso, poich\'e $T$ è normale, per il teorema spettrale \ref{t:spettrale} è addirittura sempre diagonalizzabile, quindi la tesi è immediata.
	Diverso è il caso in cui $V$ è sul campo reale.
	Prendiamo una base $\{  e_i\}_{i=1}^n$ ortonormale, e la matrice $A$ associata a $T$ in tale base.
	Consideriamo lo spazio $\C^m$ con il prodotto hermitiano standard\footnote{Intendiamo in questa dimostrazione il prodotto hermitiano come $\hermit{  a}{  b}=  a^t A\conj{  b}$, con A la matrice identità poichè è il prodotto standard.} $\hermit{}{}$, e definiamo in esso l'operatore $S\in\End(\C^m)$ tale per cui $S(  x)=A  x$.
	$S$ è autoaggiunto, infatti
	\begin{equation}
		\hermit{S(  x)}{  y}=\hermit{A  x}{  y}=  x^tA^t\conj{  y}=  x^t\conj{A  y}=\hermit{  x}{A  y}=\hermit{  x}{S(  y)},
	\end{equation}
	dato che $A$ è hermitiana quindi $A^t=\conj A$.
	Prendiamo dunque il polinomio caratteristico $\chi_T(x)=\det(A-xI)$.
	Poich\'e la matrice associata nella base canonica a $S$ è ancora $A$, risulta $\chi_T=\chi_S$.
	Ma $\chi_S\in\C[x]$ quindi ammette almeno una radice complessa, e lo stesso per $\chi_T$ anche se è in $\R[x]$, perch\'e i due coincidono.
	Se $\lambda$ è una radice di $\chi_T$, è allora un autovalore di $S$ (non è detto che lo sia di $T$, dato che i suoi autovalori sono reali).
	Allora la matrice (reale) data da $A-\lambda I$ è singolare, cioè il sistema $(A-\lambda I)  x=  0$ ammette una soluzione non nulla, che indichiamo con $  b$, che appartiene a $\R^m$.
	Posto $  b=(b_1,\dots,b_m)$, prendiamo il vettore $\valpha=\sum_{i=1}^nb_i  e_i$.
	Per come è costruito $\valpha$ si ha che $T(\valpha)=\lambda\valpha$, dunque $\lambda$ è un autovalore di $T$.
\end{proof}
Grazie a questo risultato possiamo ottenere un risultato analogo al teorema spettrale su uno spazio vettoriale reale.
Infatti una volta trovato il primo autovalore che chiamiamo $\lambda_1$, restringendo $T$ al sottospazio ortogonale all'autospazio $V_{\lambda_1}$ otteniamo ancora un endomorfismo (nel sottospazio) autoaggiunto, e possiamo ripetere il procedimento.
Questo vale chiaramente anche su spazi vettoriali complessi, quindi diamo per dimostrato il seguente teorema.
\begin{teorema}
	Sia $V$ uno spazio vettoriale reale dotato di prodotto interno.
	Se $T\in\End(V)$ è autoaggiunto, allora è diagonalizzabile.
\end{teorema}

\section{Isometrie} \label{sec:isometrie}
\begin{definizione} \label{d:isometria}
	Siano $V$ e $Z$ due spazi vettoriali sul medesimo campo, dotati rispettivamente dei prodotti interni $\intern{}{}_V$ e $\intern{}{}_Z$.
	Un'applicazione lineare $T\in\lin(V,Z)$ è un'\emph{isometria} se per ogni $  x,  y\in V$ si ha
	\begin{equation*}
		\intern{T(  x)}{T(  y)}_Z=\intern{  x}{  y}_V.
	\end{equation*}
\end{definizione}
Un'isometria dunque è un'applicazione che preserva il prodotto interno: negli spazi euclidei le isometrie in particolare preservano le distanze e gli angoli (che si possono derivare dalla norma e dal prodotto scalare).
Si può vedere subito che un'isometria è necessariamente iniettiva, dato che se $T(  x)=0_Z$ allora $0=\intern{T(  x)}{T(  x)}_Z=\intern{  x}{  x}_V=\norm{  x}^2$ e quindi $  x=0_V$ e $\Ker T=\{0_V\}$; allo stesso tempo può benissimo non essere suriettiva.
Le isometrie suriettive quindi sono anche automaticamente isomorfismi tra i due spazi\footnote{Chiameremo per brevità \emph{isomorfismi isometrici} le isometrie che sono suriettive, cioè isomorfismi.}, e in quanto tali ``portano'' delle basi di $V$ in basi di $Z$.
La peculiarietà delle isometrie è che oltre a questo preservano l'ortonormalità di queste basi, come vediamo nel seguente teorema.
\begin{teorema} \label{t:isometrie-basi-ortonormali}
	Siano $V$ e $Z$ due spazi vettoriali dotati dei rispettivi prodotti interni $\intern{}{}_V$ e $\intern{}{}_Z$, di dimensione finita e sul medesimo campo.
	Dato $T\in\lin(V,Z)$, esso è un isomorfismo isometrico se e solo se l'immagine di una base ortonormale di $V$ attraverso $T$ è una base ortonormale di $Z$.
\end{teorema}
\begin{proof}
	Cominciamo dall'ipotesi che $T$ sia un isomorfismo isometrico, e prendiamo una base ortonormale $\{  e_i\}_{i=1}^n$ di $V$.
	Sappiamo che la sua immagine $\{T(  e_i)\}_{i=1}^n$ è una base di $Z$, essendo $T$ un isomorfismo.
	Inoltre,
	\begin{equation}
		\intern{T(  e_i)}{T(  e_j)}_Z=\intern{  e_i}{  e_j}_V=\delta_{ij}
		\label{eq:dim-isometrie-basi-ortogonali1}
	\end{equation}
	dunque è anche una base ortonormale.

	Sia ora $\{  e_i\}_{i=1}^n$ una base ortonormale di $V$ tale che $\{T(  e_i)\}_{i=1}^n$ sia una base ortonormale di $Z$.
	Anche in questo caso si ha la \eqref{eq:dim-isometrie-basi-ortogonali1}.
	Prendiamo due vettori di $V$, $  x=\sum_{i=1}^nx_i  e_i$ e $  y=\sum_{i=1}^ny_i  e_i$.
	Il prodotto interno delle loro immagini è
	\begin{equation}
		\intern{T(  x)}{T(  y)}_Z=\sum_{i=1}^nx_i\conj{y_i}\intern{T(  e_i)}{T(  e_i)}_Z=\sum_{i=1}^nx_i\conj{y_i}\intern{  e_i}{  e_i}_V=\intern{  x}{  y}_V
	\end{equation}
	l'ultimo passaggio si ha per definizione di prodotto interno in basi ortonormali, si ha quindi che $T$ è un'isometria.
	La sua suriettività è evidente, infatti l'immagine di $T$ è generata da una base di $Z$, cioè è tutto lo spazio $Z$.
\end{proof}

\begin{corollario} \label{t:spazi-isometrici}
	Siano $V$ e $Z$ due spazi vettoriali dotati rispettivamente del prodotto interno $\intern{}{}_V$ e $\intern{}{}_Z$, di dimensione finita e sul medesimo campo.
	Essi sono isometrici, ossia esiste un'isometria $T\colon V\to Z$, se e solo se hanno dimensione uguale.
\end{corollario}
\begin{proof}
	Supponiamo che esista un'isometria suriettiva, per il teorema \ref{t:isometrie-basi-ortonormali}, abbiamo che basi di uno spazio sono portate in basi di un'altro spazio, quindi si arriva banalmente a concludere che le basi devono avere dimensione uguale, di conseguenza anche gli spazi. 
	
	Viceversa consideriamo due spazi che hanno uguale dimensione e verifichiamo che esista un'isometria suriettiva.
	Poniamo $T\colon \{e_i\}_{i=1}^n\to \{g_k\}_{k=1}^n$, basi ortonormali, si ha che $  x = \sum_{i=1}^n x_i e_i$ e $T(  x) = \sum_{k=1}^n x_k g_k$.
	Se la norma si preserva, allora rispetta il prodotto interno ed è un'isometria, infatti
	\begin{equation*}
		\norm{T(  x)}_Z^2 = \intern{T(x)}{T(x)}_Z = \sum_{k=1}^m x_k \conj {x_k} \intern{e_i}{e_i}_V = \intern{  x}{  x}_V = \norm {x}_V^2.
	\end{equation*}
	Quindi, si ha che esiste un'isometria suriettiva che porta basi ortonormali in basi ortonormali.
\end{proof}

\begin{teorema} \label{t:isometrie-operatori-unitari}
	Sia $V$ uno spazio vettoriale con il prodotto interno $\intern{}{}$ e $T\in\End(V)$.
	Se esiste l'aggiunto di $T$, allora esso è un'isometria se e solo se $\adj T\circ T= I_V$.
\end{teorema}
\begin{proof}
	Sia $T$ un'isometria: per ogni $  x,  y\in V$ abbiamo allora $\intern{  x}{  y}=\intern{T(  x)}{T(  y)}=\intern{  x}{\adj T\big(T(  y)\big)}$.
	Sottraendo il primo e il terzo membro otteniamo $\intern{  x}{  y}-\intern{  x}{\adj T\big(T(  y)\big)}=\intern{  x}{  y-\adj T\big(T(  y)\big)}=0$.
	Poiche la relazione vale per ogni $  x$, scegliamo $  x=  y-\adj T\big(T(  y)\big)$ ottenendo $\intern{  y-\adj T\big(T(  y)\big)}{  y-\adj T\big(T(  y)\big)}=\norm{  y-\adj T\big(T(  y)\big)}^2=0$, che è vera se e solo se $  y=\adj T\big(T(  y)\big)$, cioè $\adj T\circ T= I_V$.

	Dimostriamo ora l'inverso, partendo da $\adj T\circ T= I_V$.
	Preso un $  x\in V$ qualsiasi, $\intern{T(  x)}{T(  x)}=\intern{  x}{\adj T\big(T(  x)\big)}=\intern{  x}{  x}$, quindi è un'isometria.
\end{proof}
Per le isometrie dunque l'aggiunto è anche l'inverso dell'operatore.
\begin{teorema} \label{t:autovalori-operatori-unitari}
	Sia $V$ uno spazio vettoriale complesso di dimensione finita, dotato di un prodotto interno, e $T\in\End(V)$ unitario\footnote{Un'operatore si definisce unitario se è un'isometria nel campo complesso.}.
	Se $\lambda$ è un suo autovalore, allora $\lambda=e^{i\theta}$ per qualche $\theta\in[0,2\pi)$.
\end{teorema}
\begin{proof}
	Sia $  v\in V$ un autovettore di $T$.
	Poich\'e $T$ è unitario, $\adj T\circ T=\id_V$ per il teorema \ref{t:isometrie-operatori-unitari}, quindi
	\begin{equation}
		  v=\id_V  v=\adj T\big(T(  v)\big)=\adj T(\lambda  v)=\lambda\adj T(  v)=\lambda\conj\lambda  v=\abs{\lambda}^2  v
	\end{equation}
	e dunque $\abs{\lambda}=1$, cioè $\lambda$ sta sulla circonferenza unitaria di $\C$, e si esprime come $e^{i\theta}$ per un qualche $\theta\in\R\quot2\pi\Z$.
\end{proof}
Nel campo reale gli operatori unitari sono più semplicemente quelli per cui il loro trasposto è l'inverso, e si dicono \emph{ortogonali}.
L'esempio più noto di operatori ortogonali sono le rotazioni: in $\R^2$, una rotazione $\mathfrak R$ di un angolo $\theta$ è espressa sulla base canonica da
\begin{equation}
	\mathfrak R(\ii)=\cos\theta\ii+\sin\theta\jj\qeq \mathfrak R(\jj)=-\sin\theta\ii+\cos\theta\jj,
	\label{eq:rotazione-base}
\end{equation}
quindi la sua matrice associata è $R=\begin{bsmallmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bsmallmatrix}$.
Si vede subito che $\det R=1$ indipendentemente dall'angolo, inoltre la sua inversa è
\begin{equation}
	R^{-1}=
	\begin{bmatrix}
		\cos\theta	&\sin\theta\\
		-\sin\theta	&\cos\theta
	\end{bmatrix}
	\label{eq:rotazione-inversa-matrice}
\end{equation}
che è una rotazione di $-\theta$, ed evidentemente anche la trasposta di $R$.

% La parte seguente avrebbe davvero bisogno di una sistemata :-(
Dopo aver studiato le isometrie come applicazioni, passiamo a studiare come sono le matrici associate.
Poich\'e l'operatore composto con il suo aggiunto dà l'identità, e in una base ortonormale la matrice associata all'aggiunto è la trasposta coniugata, se $A$ è la matrice associata all'isometria $T$ in una base ortonormale allora
\begin{equation*}
	A\adj A=\adj AA=I
\end{equation*}
vale a dire che $A^{-1}=\adj A$.
Le matrici con questa proprietà, cioè tali che la loro inversa coincide con la trasposta coniugata, si chiamano \emph{unitarie}.
Esse formano un gruppo rispetto al prodotto riga per colonna, che è il \emph{gruppo unitario} e si indica con $U(n)$, dove $n$ è l'ordine delle matrici.
Nel caso $n=1$, il gruppo $U(1)$ consiste nell'insieme dei numeri complessi di modulo 1, e il prodotto riga per colonna si riduce al più semplice prodotto tra numeri complessi.
Più in generale il determinante di una matrice di $U(n)$ è un numero complesso di modulo unitario, cioè un numero della forma $e^{i\theta}$, che è anche un elemento di $U(1)$.
Il \emph{gruppo speciale unitario} $SU(n)$, sempre rispetto alla moltiplicazione tra matrici, è il sottogruppo delle matrici unitarie $n\times n$ aventi determinante 1.

Nel caso reale, il coniugio non ha effetto sulle matrici, quindi abbiamo soltanto $AA^t=A^tA=I$, o $A^{-1}=A^t$.
Queste matrici, la cui inversa coincide con la trasposta, si dicono \emph{ortogonali}.
Anch'esse formano un gruppo rispetto alla moltiplicazione tra matrici, il \emph{gruppo ortogonale} (che è in un certo senso l'analogo reale del gruppo unitario) che si indica con $O(n)$, dove $n$ è ancora l'ordine delle matrici.
Ogni matrice ortogonale ha determinante 1 oppure $-1$.
Il gruppo speciale ortogonale $SO(n)$ è il sottogruppo di $O(n)$ in cui le matrici hanno determinante 1.

